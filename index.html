<!DOCTYPE html>
<html >
<head>
  <!-- Site made with Mobirise Website Builder v4.9.1, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v4.9.1, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/logo4.png" type="image/x-icon">
  <meta name="description" content="">
  <title>Home</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
</head>
<body>
  <section class="cid-s51eCcAWZX mbr-fullscreen mbr-parallax-background" id="header2-0">

    

    <div class="mbr-overlay" style="opacity: 0.4; background-color: rgb(35, 35, 35);"></div>

    <div class="container align-center">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title mbr-bold pb-3 mbr-fonts-style display-1">INFSCI 3350 DOCTORAL SEMINAR</h1>
                <h3 class="mbr-section-subtitle align-center mbr-light pb-3 mbr-fonts-style display-2">Network Embeddings to<div>Model Science, Technology, and Economic Growth</div><br><div>Instructor: Lingfei Wu</div></h3>
                <p class="mbr-text pb-3 mbr-fonts-style display-5"><br>Acknowledgement for comments:&nbsp;Daqing He,&nbsp;Peter Brusilovsky,&nbsp;Konstantinos Pelechrinis,&nbsp;Morgan Frank,&nbsp;Malihe Alikhani.</p>
                
            </div>
        </div>
    </div>
    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="engine"><a href="https://mobirise.info/m">free site design templates</a></section><section class="cid-s51eCQOSkY mbr-fullscreen mbr-parallax-background" id="header2-1">

    

    <div class="mbr-overlay" style="opacity: 0.5; background-color: rgb(35, 35, 35);"></div>

    <div class="container align-center">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title mbr-bold pb-3 mbr-fonts-style display-1">
                    Where, when, &amp; who ?</h1>
                
                <p class="mbr-text pb-3 mbr-fonts-style display-5">Instructor: Lingfei Wu&nbsp;<br>Class No. 21891 (1050)<br>    Days and Time: Tuesday, 12:00 pm - 2:50 pm<br>    Room: Information Science Building 828&nbsp;</p>
                <div class="mbr-section-btn"><a class="btn btn-md btn-danger display-4" href="https://raw.githubusercontent.com/lingfeiwu/INFSCI_3350/master/INFSCI_3350_DOCTORAL_SEMINAR_Ling.pdf" target="_blank">Download Syllabus PDF</a></div>
            </div>
        </div>
    </div>
    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header12 cid-s51eDV57cT mbr-fullscreen mbr-parallax-background" id="header12-2">

    

    <div class="mbr-overlay" style="opacity: 0.5; background-color: rgb(35, 35, 35);">
    </div>

    <div class="container  ">
            <div class="media-container">
                <div class="col-md-12 align-center">
                    <h1 class="mbr-section-title pb-3 mbr-white mbr-bold mbr-fonts-style display-1">
                        Highlights</h1>
                    
                    

                    <div class="icons-media-container mbr-white">
                        <div class="card col-12 col-md-6 col-lg-3">
                            
                            <h5 class="mbr-fonts-style display-2">Visions/</h5>
                        </div>

                        <div class="card col-12 col-md-6 col-lg-3">
                            
                            <h5 class="mbr-fonts-style display-2">Ideas/</h5>
                        </div>

                        <div class="card col-12 col-md-6 col-lg-3">
                            
                            <h5 class="mbr-fonts-style display-2">Figures/</h5>
                        </div>

                        <div class="card col-12 col-md-6 col-lg-3">
                            
                            <h5 class="mbr-fonts-style display-2">
                                Papers/</h5>
                        </div>
                    </div>
                </div>
            </div>
    </div>

    
</section>

<section class="header5 cid-s51eEO5Xol mbr-fullscreen" id="header5-3">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1"><strong>What is network embedding?</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   Network embedding, or Graph Representation Learning, is a task of learning vector representation of nodes on networks so that we can calculate these vectors to retrieve the network similarity between nodes easily. Node similarity can be defined in many ways, including the first-order similarity (nodes are linked), the second-order similarity (nodes share neighbors), or high-order associations (e.g., nodes occupy the same level in the “hierarchy” of networks). <br><br>From this perspective, frequently used network embeddings such as “deepwalk” (Perozzi et al. 2014) or node2vec (Grover and Leskovec 2016) are only different in the definitions of node similarities, or, in natural language processing terms, strategies in retrieving the context nodes for a target node. The parameterizing and training of these models are the same (check Jure Leskovec's lecture for more details). <br><br>For this reason, our introductory seminar focuses on simple node similarities (the first and second orders) and swallow embeddings (one hidden layer) before extending to complicated similarities and deep embeddings such as TransE (Bordes et al. 2013) or Graph Neural Networks (Kipf et al., 2017). Using simple yet scalable models sets us free from overfitting and increases the likelihood of discovering universal patterns across the complex social contexts.
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eG8JGlp mbr-fullscreen" id="header5-4">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1"><strong>Why is network embedding useful?</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   Network embeddings emerge from the conjunction of complex networks (what to embed), manifold learning (how to embed), and neural networks (how to embed efficiently). We will discover how network embeddings are related to the important literature from these fields with a focus on the methodological advancement of network embeddings in <br><br>1) permitting simple models of diffusion by capturing the hidden geometry of networks; <br>2) revealing the common bases of node functions through their distributional representation; <br>3) recommending the optimal paths in networks by modeling the dependencies between nodes and links. <br><br>We will discuss how to leverage these reasoning and predictive power of network embeddings in understanding the coordination of "know-how" within and between economic agents ranging from workers, teams, companies, and cities to countries.
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eGT72Wt mbr-fullscreen mbr-parallax-background" id="header5-5">

    

    <div class="mbr-overlay" style="opacity: 0.6; background-color: rgb(35, 35, 35);">
    </div>
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-1"><strong>A “bulbs” metaphor of know-how in economies</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style display-7">
                   We use a “bulbs” metaphor to understand how talent is packed and coordinated in social systems. The limited cognition capacity of individual knowledge workers determines the limited progress of science, education, and economic growth that can be made at a time, reducing the complexity of social imagination down to its realization (Hidalgo 2015). <br><br>Teams and labor divisions are invented to go beyond individual capacity by channeling individual wisdom into knowledge and skill networks such as companies, cities, and countries. However, this massive use and production of knowledge is not perfect - not all good things scale up with population and connection. As “string lights” networks evolve to be larger and more resilient, the individual “bulbs” are getting dimmer and more replaceable - the fragmentation of scholarship and the reskilling of workers. For example, the larger teams may generate more ideas (Wuchty, Jones, and Uzzi 2007), but not necessarily more innovative ideas (Wu, Wang, and Evans 2019). <br><br>The slow diffusion of “know-how” (Hausmann et al. 2014) across experts who are trained to deliver specific tasks has become both the cause and consequence of the unscalability of integrated knowledge, slowing down the advance in science and technology and the growth of economic growth. 
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header9 cid-s51eHFTx3P mbr-fullscreen" id="header9-6">

    

    <div class="mbr-overlay" style="opacity: 0.6; background-color: rgb(35, 35, 35);">
    </div>

    <div class="container">
        <div class="media-container-column mbr-white col-lg-8 col-md-10 m-auto">
            
            <h3 class="mbr-section-subtitle align-left mbr-light pb-3 mbr-fonts-style display-2"><strong>Identifying the Moving Frontiers of Science and Technology (weeks 1-5)</strong></h3>
            <p class="mbr-text align-left pb-3 mbr-fonts-style display-7">
                \ </p>
            
        </div>
    </div>

    
</section>

<section class="header5 cid-s51eIDSV4H mbr-fullscreen" id="header5-7">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 1.  Brockmann, D., &amp; Helbing, D. (2013). The hidden geometry of complex, network-driven contagion phenomena. science, 342(6164), 1337-1342.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper proposes that commuting distance, defined as the logged inverse of the number of flight passengers between two cities, is a better measure than the geographical distance in predicting the spread of disease.
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eLsxjZX mbr-fullscreen" id="header5-8">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 2. Tenenbaum, J. B., De Silva, V., &amp; Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. science, 290(5500), 2319-2323.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper proposes to extract the “geodesic distance”, or “characteristic distance”, between nodes from high-dimensional Euclidean feature spaces by calculating the similarity distance to link nodes and then only keep local linkages.   
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eMd1VdA mbr-fullscreen" id="header5-9">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5">Week 3. Papadopoulos, F., Kitsak, M., Serrano, M. Á., Boguná, M., &amp; Krioukov, D. (2012). Popularity versus similarity in growing networks. Nature, 489(7417), 537-540.</h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">The authors propose that near-neighbor linking on a hyperbolic space replicates the long-tail degree distribution and high clustering coefficient of real-world networks, thus their model is equal to the “preferential attachment” (Barabasi 1999) and “small-world” (watts and Strogatz 1998) models combined.&nbsp;<br></p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eNBuJxJ mbr-fullscreen" id="header5-a">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 4. Tshitoyan, V., Dagdelen, J., Weston, L., Dunn, A., Rong, Z., Kononova, O., ... &amp; Jain, A. (2019). Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763), 95-98.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   The authors propose that word2vec (Mikolve 2013), a neural network linguistic model, successfully predicts the diffusion of scholars' collective attention in the knowledge space towards new chemical materials.
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eO8k0Pb mbr-fullscreen" id="header5-b">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 5. Levy, O., &amp; Goldberg, Y. (2014). Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems (pp. 2177-2185).</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">This paper proves mathematically that term-context embedding (T_i*C_j) in word2vec skip-gram negative sampling (SGNS) implicitly models pointwise mutual information (PMI) between two words.<br></p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header9 cid-s51eOQL46t mbr-fullscreen" id="header9-c">

    

    <div class="mbr-overlay" style="opacity: 0.8; background-color: rgb(35, 35, 35);">
    </div>

    <div class="container">
        <div class="media-container-column mbr-white col-lg-8 col-md-10 m-auto">
            
            <h3 class="mbr-section-subtitle align-left mbr-light pb-3 mbr-fonts-style display-2"><strong>Discovering Skill and Knowledge Atoms to Translate between Professions and Scholarships (week 6-10)</strong></h3>
            
            
        </div>
    </div>

    
</section>

<section class="header5 cid-s51ePKLZG7 mbr-fullscreen" id="header5-d">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 6. Merton, R. K. (1961). Singletons and multiples in scientific discovery: A chapter in the sociology of science. Proceedings of the American Philosophical Society, 105(5), 470-486.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper discusses the pattern of “multiple discoveries”, in which several scholars claim the same discovery independently. The author analyzed 264 multiple scientific discoveries and found them spread out across fields but concentrated in time. 
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eQq5HlP mbr-fullscreen" id="header5-e">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5">Week 7. Nissen, S. B., Magidson, T., Gross, K., &amp; Bergstrom, C. T. (2016). Publication bias and the canonization of false facts. Elife, 5, e21451.</h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   The authors model the community’s confidence in a claim as a Markov process with successive published results shifting the degree of belief.
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eRiYU0L mbr-fullscreen mbr-parallax-background" id="header5-f">

    

    <div class="mbr-overlay" style="opacity: 0.6; background-color: rgb(35, 35, 35);">
    </div>
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 8. Hope, T., Chan, J., Kittur, A., &amp; Shahaf, D. (2017, August). Accelerating innovation through analogy mining. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 235-243).</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper trains vector representations of product descriptions and suggests that the revealed analogies significantly increased people's likelihood of generating creative ideas compared to analogies retrieved by traditional methods. 
                </p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51eSTFuDC mbr-fullscreen" id="header5-g">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 9. Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems (pp. 2787-2795).&nbsp;</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper introduces the model of “TranseE”, which embeds both entities and their relations in the same Euclidean space. This allows for reasoning across the paths of knowledge graphs by multiplying corresponding vectors. “TranseE” can be viewed as a supervised version of word2vec.</p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header9 cid-s51eTXdZXG mbr-fullscreen" id="header9-h">

    

    <div class="mbr-overlay" style="opacity: 0.8; background-color: rgb(35, 35, 35);">
    </div>

    <div class="container">
        <div class="media-container-column mbr-white col-lg-8 col-md-10 m-auto">
            
            <h3 class="mbr-section-subtitle align-left mbr-light pb-3 mbr-fonts-style display-2"><strong>Designing Optimal Skills Paths for Workers, Teams, and Companies (weeks 11-14)</strong></h3>
            
            
        </div>
    </div>

    
</section>

<section class="header5 cid-s51eUGqK0K mbr-fullscreen" id="header5-i">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 11. Wu, L., Wang, D., &amp; Evans, J. A. (2019). Large teams develop and small teams disrupt science and technology. Nature, 566(7744), 378-382.&nbsp;</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper suggests that small and large teams are different in nature. Small teams ask questions and disrupt existing theories. Large teams answer questions and stabilize established paradigms.</p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51f1fwPWl mbr-fullscreen" id="header5-l">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 12. Nalisnick, E., Mitra, B., Craswell, N., &amp; Caruana, R. (2016, April). Improving document ranking with dual word embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (pp. 83-84).</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper analyzes large-scale text data and shows how term-term and term-context embeddings model the 2nd and 1st order associations, respectively.</p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51f0rJfS4 mbr-fullscreen" id="header5-k">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 13. Boudreau, K. J., Lacetera, N., &amp; Lakhani, K. R. (2011). Incentives and problem uncertainty in innovation contests: An empirical analysis. Management science, 57(5), 843-863.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper analyzes programming contest data from Topcoder and suggests that for uncertain tasks many small teams are better, whereas for tasks high in certainty a few large teams is the best.</p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="header5 cid-s51f2aH0gn mbr-fullscreen" id="header5-m">

    

    
    <div class="container">
        <div class="row justify-content-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center pb-3 mbr-fonts-style display-5"><strong>Week 14. Alabdulkareem, A., Frank, M. R., Sun, L., AlShebli, B., Hidalgo, C., &amp; Rahwan, I. (2018). Unpacking the polarization of workplace skills. Science advances, 4(7), eaao6030.</strong></h1>
                <p class="mbr-text align-center display-5 pb-3 mbr-fonts-style">
                   This paper shows the polarization in skill networks between cognitive and physical skills and reveals the projection of this polarization at the level of professions and cities and the consequential constraints to career mobility.</p>
                
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>


  <script src="assets/web/assets/jquery/jquery.min.js"></script>
  <script src="assets/popper/popper.min.js"></script>
  <script src="assets/tether/tether.min.js"></script>
  <script src="assets/bootstrap/js/bootstrap.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/parallax/jarallax.min.js"></script>
  <script src="assets/vimeoplayer/jquery.mb.vimeo_player.js"></script>
  <script src="assets/theme/js/script.js"></script>
  
  
</body>
</html>